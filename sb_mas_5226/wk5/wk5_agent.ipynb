{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox.example\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward structure should be placed in in grid\n",
    "class GridWorld:\n",
    "    def __init__(self, possible_states, transition_matrix, reward_matrix):\n",
    "        self.possible_states = possible_states  # How many places the agent can be in\n",
    "        self.transition_matrix = transition_matrix  # Rules of movement\n",
    "        self.reward_matrix = reward_matrix  # Points the agent gets for actions\n",
    "        \n",
    "        \n",
    "        n = 2\n",
    "        m = 2\n",
    "        self.size = n*m\n",
    "        self.destination = (0,0)\n",
    "        \n",
    "        \n",
    "    def __init__(self, num_actions, policy):\n",
    "        self.num_actions = num_actions  # How many choices the agent can make\n",
    "        self.policy = policy  # Agent's way of choosing actions\n",
    "        self.state = 0  # The agent starts in the first place\n",
    "    \n",
    "    def inititalizeStateSpace(self, a_pos: list, p_pos: tuple):\n",
    "        p_pos = ([1,1])\n",
    "        \n",
    "        \n",
    "        has_item = None\n",
    "        \n",
    "        a_prime = (0,0)\n",
    "        transition_matrix = np.array([\n",
    "        [[0.8, 0.2, 0.0], [0.1, 0.7, 0.2]],  # State 0\n",
    "        [[0.2, 0.6, 0.2], [0.3, 0.3, 0.4]],  # State 1\n",
    "        [[0.0, 0.3, 0.7], [0.5, 0.3, 0.2]]   # State 2\n",
    "    ])\n",
    "        a_state = {[has_item, a_pos, p_pos]}\n",
    "        a_state_prime = {[None, None, None]}\n",
    "        \n",
    "        for dx, dy in transition_matrix:\n",
    "            if a_state(0) == True:\n",
    "                return 1\n",
    "            else: \n",
    "                for dy in transition_matrix:\n",
    "                    for dx in dy:\n",
    "                        new_has_item = None\n",
    "                        new_a_pos = np.random.randint(0,4)\n",
    "                        new_p_pos = p_pos\n",
    "                        a_state_prime = {[new_has_item, new_a_pos, new_p_pos]}\n",
    "                        a_state.update(a_state_prime)\n",
    "                    print(a_state_prime)\n",
    "\n",
    "            \n",
    "    \n",
    "    def choose_action(self):\n",
    "        return self.policy[self.state]  # Follows the policy for the current state\n",
    "    \n",
    "    def step(self):\n",
    "        action = self.choose_action()  # Pick an action based on the policy\n",
    "        probabilities = self.transition_matrix[self.state][action]  # Get movement rules\n",
    "        self.state = np.random.choice(self.num_states, p=probabilities)  # Move to a new state\n",
    "        reward = self.reward_matrix[self.state]  # Get the reward for the new state\n",
    "        return self.state, reward  # Return where the agent is now and its reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting up an environment\n",
    "num_states = 1  # Three places the agent can be in\n",
    "num_actions = 2  # Two choices at each place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = np.array([1, -1, 2])  # State 0 gives 1 point, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transition_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      2\u001b[0m         [[\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]],  \u001b[38;5;66;03m# State 0\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         [[\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], [\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m]],  \u001b[38;5;66;03m# State 1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         [[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]]   \u001b[38;5;66;03m# State 2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     ])  \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m(GridWorld):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_actions, policy):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# example trans matrix\n",
    "\n",
    "\n",
    "class Agent(GridWorld):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [0, 1, 0]  # In state 0, take action 0, in state 1, take action 1, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = Agent.inititalizeStateSpace((0,0), (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        Agent.__init__\n",
    "        GridWorld.__init__\n",
    "\n",
    "    def reset(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fit5226",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
